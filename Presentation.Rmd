---
title: "Social Power NBA Dataset"
author: "Julia Wilkins & Adrian Naranjo"
---

<br>

## Data

* Found on Kaggle: https://www.kaggle.com/noahgift/social-power-nba
* Contains performance, salary, Wikipedia pageviews, and Twitter RT/Favorite data for players in the 2016-2017 season across 30 teams
* Performance statistics: $n = 446, p = 36$
* Social media statsitcs: $n = 239, p = 3$


## We asked, can we predict...

1. Salary from performance statistics?
2. Points per game based on other performance metrics?
3. Salary based on social media stats?

<br>

```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(tidyverse)
library(modelr)
library(boot)
library(leaps)
library(gridExtra)
library(gam)
library(knitr)
library(kableExtra)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}

# Load Data
team_valuations <- read_csv('raw_data/nba_2017_team_valuations.csv')
att_val_elo <- read_csv('raw_data/nba_2017_att_val_elo.csv')
salary <- read_csv('raw_data/nba_2017_salary.csv') %>%
  mutate(SALARY2 = (SALARY / 1000000)) 
players_with_stats_combined <- read_csv('raw_data/nba_2017_players_stats_combined.csv')
players_with_salary_wiki_twitter <- read_csv('raw_data/nba_2017_players_with_salary_wiki_twitter.csv')

# Aggregates social media statistics at a team level
# Removes players which played for more than one team
team_twitter_wiki <- drop_na(players_with_salary_wiki_twitter) %>%
  group_by(TEAM) %>%
  summarise(total_pageviews = sum(PAGEVIEWS)
            ,total_twitter_favorite = sum(TWITTER_FAVORITE_COUNT)
            ,total_twitter_retweet = sum(TWITTER_RETWEET_COUNT)) %>%
  filter(!str_detect(TEAM, '/'))
```

## Salary EDA: Distribution of Individual Salaries
### Right Skewed: Keep this in mind. We'll cut the tail later.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Display histogram of salaries... not surprisingly, right skewed
salary %>% 
  ggplot(aes(SALARY2, labels = TRUE)) +
  geom_histogram(binwidth = 2.5, color = "black", fill = "white") +
  stat_bin(geom="text", aes(label = ..count..), binwidth = 2.5, position=position_stack(vjust=0.5)) +
  labs(x = 'Salary (Millions)', y = 'Number of Players')
```

#### Note: bins are $2.5 mil

<br>
<br>

### So, where do we want to cut? ...Somewhere between $10 - $15 mil looks good.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Find a quantiles
quantiles <- quantile(salary$SALARY, seq(0, 1, .1)) 

quantiles <- tibble(Percentile = names(quantiles)
       ,Salary = quantiles)

quantiles %>% 
  kable("html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```
<br>
<br>

## Salary by Position
### If you're going to play, try playing center. Otherwise, nothing too interesting.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Display salaries by position 
# Removed rows that only display a position a 'F' or 'G'
salary %>% 
  filter(POSITION %in% c('C', 'PF', 'PG', 'SF', 'SG')) %>% 
  ggplot(aes(POSITION, SALARY)) +
  geom_boxplot() +
  labs(x = 'Salary', y = 'Position')
  # ggtitle("Salary By Position")
```
<br>
<br>

## Median Salary by Position
### In case you're unsure center is the position for you (hint: it is)

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Calculate average salary/position
salary %>% 
    rename(Position = POSITION) %>% 
    filter(Position %in% c('C', 'PF', 'PG', 'SF', 'SG')) %>% 
    group_by(Position) %>% 
    summarise(Avg_Salary = round(median(SALARY) / 1000000, digits = 2)) %>%
    kable("html") %>%
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```
<br>
<br>

## Salary by Age
### Increases until leveling off and [slightly] declining. Makes sense: better players keep playing.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Exploring salary vs. age
players_with_stats_salary <- inner_join(players_with_stats_combined, salary, by = c('PLAYER' =  'NAME'))
players_with_stats_salary <- select(players_with_stats_salary, -POSITION.y, -TEAM.y, -SALARY2)
players_with_stats_salary <- drop_na(players_with_stats_salary)

# Correlation between Age and Salary. Took out outliers.
players_with_stats_salary %>% 
  filter(AGE >= 20, AGE <= 35) %>% 
  ggplot(aes(x = AGE, y = SALARY)) +
  geom_point() +
  geom_smooth() 
 # ggtitle("Age vs. Individual Salary")
```
<br>
<br>

## Player Rating by Salary
### Obvious conclusion: the better you are, the better you get paid (generally!)

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Correlation between between Player Effectiveness and Salary
ggplot(players_with_stats_salary, aes(x = PIE, y = SALARY)) +
  geom_point() +
  geom_smooth() 
  # ggtitle("Player Effectiveness vs. Salary")
```

<br>
<br>
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# ggplot(team_twitter_wiki) +
#   geom_point(aes(total_pageviews, TEAM), color = 'red') +
#   geom_point(aes(total_twitter_favorite, TEAM), color = 'blue') +
#   geom_point(aes(total_twitter_retweet, TEAM), color = 'black') +
#   labs(x = 'Social Media Statistics', y = 'Team')
# ggtitle("Social Media Stats per Team\n\n Red = WikiPageviews, Black = Twitter Retweets, Blue = Twitter Favorites")
```


# Question 1: Can we predict **salary** from performance statistics? 
 <br>
 <br>
 
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Create new dataset
players_with_stats_salary <- inner_join(players_with_stats_combined, salary, by = c('PLAYER' =  'NAME'))
players_with_stats_salary <- select(players_with_stats_salary, -POSITION.y, -TEAM.y, -SALARY2)
players_with_stats_salary <- drop_na(players_with_stats_salary)
```

## First, which predictors do we want?

```{r, message=FALSE, echo=FALSE, warning=FALSE}
names(players_with_stats_salary)

stats_salary_data <- players_with_stats_salary %>% 
  # Because each statistic below indicates a different model has the 
  # smallest test error, this line creates a smaller dataset for comparison  
  select(-PLAYER, -X1, -POSITION.x, -TEAM.x, -RPM, -Rk, -`FG%`, -`3P%`, -`2P%`, -`eFG%`,
         -`FT%`, -TRB, -ORPM, -DRPM, -WINS_RPM, -PLAYER, -X1, -MP) %>%
  # this is the original select
  # select(-PLAYER, -X1, -POSITION.x, -TEAM.x, -RPM) %>% 
  filter(SALARY <= quantile(salary$SALARY, .95))
```
<br> 

## Including all predictors led to issues (hello, collinearity). So, we got rid of some (e.g. Offensive Rebounds) and kept others (e.g. Total Rebounds).

#### Note: to be precise: we eliminated 18 of the 40 variables

<br>
<br>

## But, we'd still prefer less variables: Best Subset Elimination
### No clear consensus.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Create variable to hold the number of predictors
num_predictors <- dim(stats_salary_data)[2] - 1

# Perform best-subsets 
stats_salary_model <- regsubsets(SALARY ~ .
                                 ,data = stats_salary_data
                                 ,nvmax = num_predictors)

stats_salary_summary <- summary(stats_salary_model)

# Create tibble which contains data from results object
stats_salary_results <- tibble(num_pred = 1:num_predictors
                               ,rss = stats_salary_summary$rss
                               ,rsquared = stats_salary_summary$rsq
                               ,adj_rsquared = stats_salary_summary$adjr2
                               ,cp = stats_salary_summary$cp
                               ,bic = stats_salary_summary$bic)


# RSS
plot1_q1 <- stats_salary_results %>% 
  ggplot(aes(num_pred, rss)) + 
  geom_point() +
  geom_vline(aes(xintercept = which.min(stats_salary_results$rss)), color = 'red') +
  xlab('Number of Predictors') +
  ylab('RSS')

# ADJ R-SQUARED
plot2_q1 <- stats_salary_results %>% 
  ggplot(aes(num_pred, adj_rsquared)) + 
  geom_point() +
  geom_vline(aes(xintercept = which.max(stats_salary_results$adj_rsquared)), color = 'red') +
  xlab('Number of Predictors') +
  ylab('Adj R-squared')

# CP
plot3_q1 <- stats_salary_results %>% 
  ggplot(aes(num_pred, cp)) + 
  geom_point() +
  geom_vline(aes(xintercept = which.min(stats_salary_results$cp)), color = 'red') +
  xlab('Number of Predictors') +
  ylab('Cp')

# BIC
plot4_q1 <- stats_salary_results %>% 
  ggplot(aes(num_pred, bic)) + 
  geom_point() +
  geom_vline(aes(xintercept = which.min(stats_salary_results$bic)), color = 'red') +
  xlab('Number of Predictors') +
  ylab('BIC')

# Each of the measures comes up with vastly different number of 
# predictors... 
grid.arrange(plot1_q1, plot2_q1, plot3_q1, plot4_q1, nrow = 2, ncol = 2)
```
<br>
<br>
<br>

## Ok. Let's build a bunch of models (Yay!)

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Create training and test data sets
training_data_q1 <- stats_salary_data %>% sample_frac(.8)
test_data_q1 <- setdiff(stats_salary_data, training_data_q1)

# Fit model using the coefficients above and add predictions to test data
salary_model4 <- lm(SALARY ~ AGE + `2PA` + MPG + W, data = training_data_q1)
salary_model5 <- lm(SALARY ~ AGE + `2P` + `2PA` + MPG + W, data = training_data_q1)
salary_model6 <- lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W, data = training_data_q1)
salary_model7 <- lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W + TOV, data = training_data_q1)
salary_model8 <- lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W + TOV + STL, data = training_data_q1)
salary_model9 <- lm(SALARY ~ AGE + FGA + `2P` + `3PA` + ORB + POINTS + W + TOV + MPG, data = training_data_q1)
salary_model10 <- lm(SALARY ~ AGE + FGA + `3PA` + `2P` + `2PA` + ORB + TOV + POINTS + MPG + W, data = training_data_q1)

test_data_q1 <- add_predictions(test_data_q1, salary_model4, var = 'pred_lm4')
test_data_q1 <- add_predictions(test_data_q1, salary_model5, var = 'pred_lm5')
test_data_q1 <- add_predictions(test_data_q1, salary_model6, var = 'pred_lm6')
test_data_q1 <- add_predictions(test_data_q1, salary_model7, var = 'pred_lm7')
test_data_q1 <- add_predictions(test_data_q1, salary_model8, var = 'pred_lm8')
test_data_q1 <- add_predictions(test_data_q1, salary_model9, var = 'pred_lm9')
test_data_q1 <- add_predictions(test_data_q1, salary_model10, var = 'pred_lm10')

salary_model_gamma <- glm(SALARY ~ AGE + `2P` + `2PA` + ORB + MPG, data = training_data_q1, family = Gamma(link = "log"))
test_data_q1 <- mutate(test_data_q1, pred_glm = predict(salary_model_gamma, test_data_q1, type = 'response'))

text_tbl <- data.frame(
  NumberOfPredictors = c("4", "5", "6", "7", "8", "9", "10", "5 (Gamma)"),
  Predictors = c(
    "Age, 2PA, MPG, W",
    "Age, 2PA, MPG, W, 2P", 
    "Age, 2PA, W, 2P, ORB, POINTS",
    "Age, 2PA, W, 2P, ORB, POINTS, TOV",
    "Age, 2PA, W, 2P, ORB, POINTS, TOV, STL",
    "Age, W, 2P, ORB, POINTS, TOV, MPG, 3PA, FGA",
    "Age, W, 2P, ORB, POINTS, TOV, MPG, 3PA, FGA, 2PA",
    "Age, 2PA, 2P, 2PA, ORB, MPG"
  )
)

kable(text_tbl, "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "30em")
```
<br>
<br>

## Actual versus Predicted Plots
### Not much of a difference, at least visually

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Actual vs Predicted for linear model
plot_lm4 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm4)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm4), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("4 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm5 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm5)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm5), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("5 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm6 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm6)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm6), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("6 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm7 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm7)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm7), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("7 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm8 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm8)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm8), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("8 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm9 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm9)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm9), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("9 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_lm10 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_lm10)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_lm10), method = 'lm') +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("10 Predictors") +
  theme(plot.title = element_text(hjust = 0.5))

plot_glm <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_glm)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_glm), method = 'lm', color = "red") +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("5 (Gamma)") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(plot_lm4, plot_lm5, plot_lm6, plot_lm7, plot_lm8, plot_lm9, plot_lm10, plot_glm, nrow = 4, ncol = 2)
```
<br>
<br>

## Let's be more exact: what's the Test MSE
### Keep in mind, salaries are in the millions!

```{r, message=FALSE, echo=FALSE, warning=FALSE}
tibble(Model4 = rmse(salary_model4, test_data_q1)
     ,Model5 = rmse(salary_model5, test_data_q1)
     ,Model6 = rmse(salary_model6, test_data_q1)
     ,Model7 = rmse(salary_model7, test_data_q1)
     ,Model8 = rmse(salary_model8, test_data_q1)
     ,Model9 = rmse(salary_model9, test_data_q1)
     ,Model10 = rmse(salary_model10, test_data_q1)
     ,Gamma = rmse(salary_model_gamma, test_data_q1)) %>% 
kable("html") %>%
kable_styling(position = "left")
```
<br>
<br>

## How about K-Fold CV (using 5 folds)?

```{r, message=FALSE, echo=FALSE, warning=FALSE}
stats_salary10fold <- stats_salary_data %>% 
  crossv_kfold(5, id = 'fold') %>% 
  mutate(train = map(train, as_tibble)) %>% 
  mutate(lm_model4 = map(train, ~ lm(SALARY ~ AGE + `2PA` + MPG + W, data = .))
         ,lm_model5 = map(train, ~ lm(SALARY ~ AGE + `2P` + `2PA` + MPG + W, data = .))
         ,lm_model6 = map(train, ~ lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W, data = .))
         ,lm_model7 = map(train, ~ lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W + TOV, data = .))
         ,lm_model8 = map(train, ~ lm(SALARY ~ AGE + `2P` + `2PA` + ORB + POINTS + W + TOV + STL, data = .))
         ,lm_model9 = map(train, ~ lm(SALARY ~ AGE + FGA + `2P` + `3PA` + ORB + POINTS + W + TOV + MPG, data = .))
         ,lm_model10 = map(train, ~ lm(SALARY ~ AGE + FGA + `3PA` + `2P` + `2PA` + ORB + TOV + POINTS + MPG + W, data = .))
         ,glm_model = map(train
                          , ~ glm(SALARY ~ AGE + `2P` + `2PA` + ORB + MPG, data = .
                                  , family = Gamma(link = "log"))))

rmse_lm4 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model4, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm5 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model5, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm6 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model6, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm7 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model7, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm8 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model8, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm9 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model9, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_lm10 <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$lm_model10, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

rmse_glm <- stats_salary10fold %>% 
  mutate(rmse = map2_dbl(stats_salary10fold$glm_model, stats_salary10fold$test, rmse)) %>%
  summarise(mean_rmse = mean(rmse))

tibble(Model4 = rmse_lm4$mean_rmse
                       ,Model5 = rmse_lm5$mean_rmse
                       ,Model6 = rmse_lm6$mean_rmse
                       ,Model7 = rmse_lm7$mean_rmse
                       ,Model8 = rmse_lm8$mean_rmse
                       ,Model9 = rmse_lm9$mean_rmse
                       ,Model10 = rmse_lm10$mean_rmse
                       ,Gamma = rmse_glm$mean_rmse) %>% 
kable("html") %>%
kable_styling(position = "left")
```
<br>
<br>

## Are we better suited with a non-linear model?
### Do any patterns appear while ploting prominent variables against salary

```{r, message=FALSE, echo=FALSE, warning=FALSE}
plot_nonlinear1 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = AGE, y = SALARY))
  # geom_smooth(data = stats_salary_data, aes(x = AGE, y = SALARY), method = 'lm')

plot_nonlinear2 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = `2P`, y = SALARY))
  # geom_smooth(data = stats_salary_data, aes(x = `2P`, y = SALARY), method = 'lm')

plot_nonlinear3 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = `2PA`, y = SALARY))
  # geom_smooth(data = stats_salary_data, aes(x = `2PA`, y = SALARY), method = 'lm')

plot_nonlinear4 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = ORB, y = SALARY)) 
  # geom_smooth(data = stats_salary_data, aes(x = ORB, y = SALARY), method = 'lm')

plot_nonlinear5 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = POINTS, y = SALARY))
  # geom_smooth(data = stats_salary_data, aes(x = POINTS, y = SALARY), method = 'lm')

plot_nonlinear6 <- ggplot() +
  geom_point(data = stats_salary_data, aes(x = W, y = SALARY)) 
  # geom_smooth(data = stats_salary_data, aes(x = W, y = SALARY), method = 'lm')

grid.arrange(plot_nonlinear1, plot_nonlinear2, plot_nonlinear3, plot_nonlinear4, plot_nonlinear5, plot_nonlinear6
             , nrow = 3, ncol = 2)
```
<br>
<br>

## Attempt at Non-Linear Models (Local Regression)
### Yeah.... No. Not conclusive.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
local_reg_model1 <- loess(SALARY ~ AGE + MPG + W, data = training_data_q1)
local_reg_model2 <- loess(SALARY ~ AGE + MPG + W + POINTS, data = training_data_q1)

test_data_q1 <- mutate(test_data_q1, pred_local1 = predict(local_reg_model1, test_data_q1, type = 'response'))
test_data_q1 <- mutate(test_data_q1, pred_local2 = predict(local_reg_model2, test_data_q1, type = 'response'))

nonlinear_plot1 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_local1)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_local1)) +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("Age + MPG + W") +
  theme(plot.title = element_text(hjust = 0.5))

nonlinear_plot2 <- ggplot() +
  geom_point(data = test_data_q1, aes(x = SALARY, y = pred_local2)) +
  geom_smooth(data = test_data_q1, aes(x = SALARY, y = pred_local2)) +
  labs(x = 'Actual', y = 'Predicted') +
  ggtitle("Age + MPG + W + Points") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(nonlinear_plot1, nonlinear_plot2, nrow = 2, ncol = 1)
```
<br>
<br>

```{r, message=FALSE, echo=FALSE, warning=FALSE}
rmse_summary <- tibble(NonLinear_3P = rmse(local_reg_model1, test_data_q1)
          ,NonLinear_4P= rmse(local_reg_model2, test_data_q1)) 

rmse_summary %>% 
  kable("html") %>%
  kable_styling(full_width = F, position = "left")
```

<br>

# Question 2: **Points per game** based on other performance metrics?


```{r, message=FALSE, echo=FALSE}
# Adding salary to performance data
players_with_stats_salary <- inner_join(players_with_stats_combined, salary
                                        , by = c('PLAYER' =  'NAME'))
players_with_stats_salary <- select(players_with_stats_salary, -POSITION.y, -TEAM.y, -SALARY2)
players_with_stats_salary <- drop_na(players_with_stats_salary)

```

<br>

### Below we take a look at 3P (black), FT (blue), FG (green), 2P (pink) relative to overall points per game:

* Choose to use 4 predictors: 3P, FT, FG, 2P because of statistical significance
  + Tried using regsubsets but there were a lot of problems with collinearities
  + Still found number of predictors to be 3-4 (predictors above)
* "Points" is the player's average number of PPG, whereas these statistics are **quantity** of those shots made
* Less 3's made overall, still trending upward with point total however

<br>

```{r, message=FALSE, echo=FALSE}
eda_plot <- ggplot() +
  geom_point(data = players_with_stats_salary, aes(x = `3P`, y = POINTS, color="3P")) +
  geom_point(data = players_with_stats_salary, aes(x = FT, y = POINTS, color="FT")) +
  geom_point(data = players_with_stats_salary, aes(x = FG, y = POINTS, color="FG")) +
  geom_point(data = players_with_stats_salary, aes(x = `2P`, y = POINTS, color="2P")) +
  scale_color_manual("", breaks=c("3P", "FT", "FG", "2P"), values=c("3P"="green", "FT"="blue", "FG"="pink", "2P"="black")) +
  xlab("Performance Statistics Points") + ylab("Points per Game") +
  ggtitle("Points Relative to other Performance Stats") + theme(plot.title = element_text(hjust = 0.5))

eda_plot

```

<br>

### Create a simple linear model

* Validation set approach with 0.75/0.25 train/test split
* Simple linear regression model with 4 predictors does very well
* Is this because the predictors are correlated **too well** with the response?

<br>

```{r, message=FALSE, echo=FALSE, warning=TRUE}
# Make train and test sets
train_set_points <- players_with_stats_salary %>%
  sample_frac(0.75, replace=FALSE)

test_set_points <- players_with_stats_salary %>%
  setdiff(train_set_points)

# THIS MODEL IS TOO GOOD!!!!!!
ppg_model <- lm(POINTS ~ FT + `3P` + FG + `2P`, data=train_set_points)

# Add predicted salary to test set
test_set_points <- add_predictions(test_set_points, ppg_model)

# Plots of predicted vs. actual for points based on those stats
points_predicted_plot <- ggplot() +
  geom_point(data = test_set_points, aes(x = seq(1:dim(test_set_points)[1]), y = POINTS), color="red") +
  geom_point(data = test_set_points, aes(x = seq(1:dim(test_set_points)[1]), y = pred), color="blue") +
  geom_smooth(data = test_set_points, aes(x = seq(1:dim(test_set_points)[1]), y = pred)) +
  xlab("Player Index") + ylab("Points") + ylim(0, range(test_set_points$pred)[2]) +
  ggtitle("Points Predicted by other Performance Stats") + theme(plot.title = element_text(hjust = 0.5))

# points_predicted_plot
# coef(ppg_model)

# Plot actual vs. predicted points
actual_v_pred_points <- ggplot(data = test_set_points, aes(x = POINTS, y = pred)) +
  geom_point() + geom_smooth() +
  xlab("Actual PPG") + ylab("Predicted PPG") + 
  ggtitle("Actual vs Predicted Points Per Game") + theme(plot.title = element_text(hjust = 0.5))
actual_v_pred_points

```
<br>

### Almost too perfect... RMSE of just 0.075. Let's check to make sure that "Points" isn't simply just the sum of our predictors (-FG) multiplied by their point values. It's close, but not quite there. This is still unsettling, and probably not that useful.

```{r, message=FALSE, echo=FALSE}
test_set_points <- test_set_points %>%
  mutate(summer = 1*FT+3*`3P`+2*`2P`)

mini_table <- tibble(original_points = (test_set_points$POINTS)[1:10])
mini_table <- mini_table %>%
  mutate(pred_points = round((test_set_points$pred)[1:10], 3),
         sum_col = (test_set_points$summer)[1:10])

mini_table %>%
  kable("html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

<br>

### For good measure, let's check out predicting points based on field goals alone - the make-up of shots that a person can have to generate the same number of points can vary so let's see if it learns this.

<br>

```{r, message=FALSE, warning=FALSE, echo=FALSE}
ppg_model_fg <- lm(POINTS ~ FG, data=train_set_points)

# Add predicted salary to test set
test_set_points_fg <- add_predictions(test_set_points, ppg_model_fg)

# Plots of predicted vs. actual for points based on those stats
points_predicted_plot_fg <- ggplot() +
  geom_point(data = test_set_points_fg, aes(x = seq(1:dim(test_set_points_fg)[1]), y = POINTS), color="red") +
  geom_point(data = test_set_points_fg, aes(x = seq(1:dim(test_set_points_fg)[1]), y = pred), color="blue") +
  geom_smooth(data = test_set_points_fg, aes(x = seq(1:dim(test_set_points_fg)[1]), y = pred)) +
  xlab("Player Index") + ylab("Points") + ylim(0, range(test_set_points_fg$pred)[2]) +
  ggtitle("Points Predicted by other Performance Stats") + theme(plot.title = element_text(hjust = 0.5))

# Plot actual vs. predicted points
actual_v_pred_points_fg <- ggplot(data = test_set_points_fg, aes(x = POINTS, y = pred)) +
  geom_point() + geom_smooth() +
  xlab("Actual PPG") + ylab("Predicted PPG") + 
  ggtitle("Actual vs Predicted Points Per Game - Field Goals as Only Predictor") + theme(plot.title = element_text(hjust = 0.5))
actual_v_pred_points_fg


```

#### This feels a little better, and our test RMSE is now 1.15.

<br>
<br>

# Question 3: **Salary** based on social media stats?

### Weak positive correlation between salary and wikipedia pageviews with outliers 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# New tibble that only has player name, salary, wiki stats, twitter stats
players_wiki_twitter <- select(players_with_salary_wiki_twitter, PLAYER)
players_wiki_twitter <- players_wiki_twitter %>%
  mutate(index = players_with_salary_wiki_twitter$X1,
         salary = players_with_salary_wiki_twitter$SALARY_MILLIONS,
         wiki_views = players_with_salary_wiki_twitter$PAGEVIEWS,
         twitter_fav = players_with_salary_wiki_twitter$TWITTER_FAVORITE_COUNT,
         twitter_RT = players_with_salary_wiki_twitter$TWITTER_RETWEET_COUNT)

wiki_eda_plot <- ggplot() +
  geom_point(data = players_wiki_twitter, aes(x = wiki_views, y = salary)) + geom_smooth() +
  xlab("Wikipedia Page Views") + ylab("Salary") + 
  ggtitle("Salary vs. WikiViews") + theme(plot.title = element_text(hjust = 0.5))

wiki_eda_plot

```

<br>

### Make simple linear model for predicting **salary** based on Wikipedia pageviews alone, combined Twitter statistics, and then all 3 predictors.

* Validation set approach with 0.75/0.25 train/test split

<br>
```{r, message=FALSE, echo=FALSE, warning=FALSE}

# drop rows with missing social media stats
players_wiki_twitter <- drop_na(players_wiki_twitter)

# Explore which predictors are statistically significant
exploratory_mod <- players_wiki_twitter %>%
  select(-PLAYER) %>%
  select(-index) %>%
  lm(salary ~., data = .)

# summary(exploratory_mod)
# Observe that wikipedia views, twitter favorites, and twitter retweets are
# all statistically significant - twitter RT seems to be a bit less significant than the others

# VALIDATION SET APPROACH
# First make train and test sets
train_set <- players_wiki_twitter %>%
  sample_frac(0.75, replace=FALSE)

test_set <- players_wiki_twitter %>%
  setdiff(train_set)

# Make linear regression model
# Let's try just using wikipedia page views alone first
glm_train <- lm(salary ~ wiki_views, data=train_set)

test_set <- add_predictions(test_set, glm_train)
test_set <- test_set %>% 
  mutate(salary_from_wiki = pred) %>%
  select(-pred)

# Quick plot of predicted vs. actual values shows us that using only Wiki is not useful
# these plots are NOT USEFUL - just looking at the predicted vs. actual
wiki_only_plot <- ggplot() +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary)) +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_wiki), color=
               "red") +
  geom_smooth(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_wiki), color=
                "red") +
  xlab("Player Index") + ylab("Salary") + 
  ggtitle("Salary vs. WikiViews") + theme(plot.title = element_text(hjust = 0.5))

# actual vs. predicted
# SHOW THIS TO SHOW HOW IT DIDN'T WORK WELL
wiki_actual_v_pred <- ggplot(data = test_set, aes(x = salary, y = salary_from_wiki)) +
  geom_point() + geom_smooth() +
  xlab("Actual Salary") + ylab("Predicted Salary") + 
  ggtitle("WikiViews") + theme(plot.title = element_text(hjust = 0.5))
#````````````````````````````````````````````````````````````

# Try model using just twitter stats, no page-view stats
twitter_only_model <- lm(salary ~ twitter_fav + twitter_RT, data=train_set)

# Add predicted salary 
test_set <- add_predictions(test_set, twitter_only_model)
test_set <- test_set %>% 
  mutate(salary_from_twitter = pred) %>%
  select(-pred)

# Calculate root mean squared error 
# rmse(twitter_only_model, test_set)

# Quick plot of predicted vs. actual values shows us that using only Wiki is not useful
# these plots are NOT USEFUL - just looking at the predicted vs. actual
twitter_only_plot <- ggplot() +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary)) +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_twitter), color=
               "green") +
  geom_smooth(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_twitter), color=
                "green") +
  xlab("Player Index") + ylab("Salary") +  ylim(0, range(test_set$salary_from_wiki)[2]) +
  ggtitle("Salary vs. Twitter Stats") + theme(plot.title = element_text(hjust = 0.5))

twitter_actual_v_pred <- ggplot(data = test_set, aes(x = salary, y = salary_from_twitter)) +
  geom_point() + geom_smooth() +
  xlab("Actual Salary") + ylab("Predicted Salary") + 
  ggtitle("Twitter Stats") + theme(plot.title = element_text(hjust = 0.5))
#````````````````````````````````````````````````````````````````````

# Those models are bad - let's use all three predictors next!
# Make linear regression model
# Let's try just using wikipedia page views alone first
salary_vs_media_model <- lm(salary ~ wiki_views + twitter_fav + twitter_RT, data=train_set)

# Add predicted salary 
test_set <- add_predictions(test_set, salary_vs_media_model)
test_set <- test_set %>% 
  mutate(salary_from_media = pred) %>%
  select(-pred)

# Calculate root mean squared error 
# rmse(salary_vs_media_model, test_set)

# Plots of predicted vs. actual with twitter and wikipedia
media_salary_plot <- ggplot() +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary)) +
  geom_point(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_media), color="blue") +
  geom_smooth(data = test_set, aes(x = seq(1:dim(test_set)[1]), y = salary_from_media)) +
  xlab("Player Index") + ylab("Salary") + ylim(0, range(test_set$salary_from_wiki)[2]) +
  ggtitle("Salary vs. Both") + theme(plot.title = element_text(hjust = 0.5))

all_social_actual_v_pred <- ggplot(data = test_set, aes(x = salary, y = salary_from_media)) +
  geom_point() + geom_smooth() +
  xlab("Actual Salary") + ylab("Predicted Salary") + 
  ggtitle("All Media Stats") + theme(plot.title = element_text(hjust = 0.5))

# Plot side-by-side
grid.arrange(wiki_only_plot, twitter_only_plot, media_salary_plot, nrow = 1, ncol = 3)
grid.arrange(wiki_actual_v_pred, twitter_actual_v_pred, all_social_actual_v_pred, nrow=1, ncol=3)
```

<br>

#### Compare Test RMSE of each model: 

<br>
(Note: RMSE is scaled in millions (i.e. 6.7 million))
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Gather RMSE and show
rmse_tib <- tibble(wiki_rmse = rmse(glm_train, test_set))
rmse_tib <- rmse_tib %>%
  mutate(twitter_rmse = rmse(twitter_only_model, test_set),
         media_rmse = rmse(salary_vs_media_model, test_set))
rmse_tib %>%
  kable("html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

<br>

### These results are not great, and intuitively Wikipedia pageviews would be the only predictor we would expect to really correlate.





<!-- # Question 4: Team valuations from individual salaries?  -->
<!--  <br> -->
<!--  <br> -->

<!-- ## Valuations by Team -->
<!-- ### As expected, teams in bigger, more popular cities are generally are worth more -->

<!-- ```{r, message=FALSE, echo=FALSE, warning=FALSE}  -->
<!-- ggplot(team_valuations, aes(x = VALUE_MILLIONS -->
<!--                             ,y = TEAM -->
<!--                             ,size = VALUE_MILLIONS -->
<!--                             ,color = VALUE_MILLIONS)) + -->
<!-- geom_point() + -->
<!-- labs(x = 'Team Value', y = 'Team Name') -->
<!-- ``` -->
<!-- <br> -->
<!-- <br> -->

<!-- ## Valuation by Attendance -->
<!-- ### There looks to be some relationship between Attendance and Valuation -->

<!-- ```{r, message=FALSE, echo=FALSE, warning=FALSE}  -->
<!-- # Shows attendance and team value are correlated -->
<!-- ggplot(att_val_elo, aes(x = VALUE_MILLIONS -->
<!--                         ,y = AVG -->
<!--                         ,size = VALUE_MILLIONS -->
<!--                         ,color = VALUE_MILLIONS)) +  -->
<!--   geom_smooth(color = 'brown') + -->
<!--   geom_point() + -->
<!--   labs(y = 'Attendance', x = 'Team Value') -->
<!-- ``` -->
<!-- <br> -->
<!-- <br> -->

<!-- ### To be exact... -->

<!-- ```{r, message=FALSE, echo=FALSE, warning=FALSE}  -->
<!-- tibble(Correlation = cor(att_val_elo$VALUE_MILLIONS, att_val_elo$AVG)) %>%  -->
<!--   kable("html") %>%  -->
<!--   kable_styling(full_width = F, position = "left") -->
<!-- ``` -->
<!-- <br> -->
<!-- <br> -->

<!-- ## Quickly: remove highly correlated predictors, perform forward elmination... -->

<!-- ```{r, message=FALSE, echo=FALSE, warning=FALSE}  -->
<!-- salary$TEAM <- str_replace(salary$TEAM, 'LA Clippers', 'Los Angeles Clippers') -->

<!-- # Same as the above but with extra fields -->
<!-- salary_valuations_by_team2 <- left_join(salary, players_with_stats_combined, by = c('NAME' = 'PLAYER')) %>%  -->
<!--   group_by(TEAM.x) %>%  -->
<!--   summarise(total_salary = sum(SALARY2, na.rm = TRUE) -->
<!--             ,median_salary = median(SALARY2, na.rm = TRUE) -->
<!--             ,mean_salary = mean(SALARY2, na.rm = TRUE) -->
<!--             ,low_salary = min(SALARY2, na.rm = TRUE) -->
<!--             ,high_salary = max(SALARY2, na.rm = TRUE) -->
<!--             ,avg_age = median(AGE, na.rm = TRUE) -->
<!--             ,avg_FG = median(FG, na.rm = TRUE) -->
<!--             ,avg_FGA = median(FGA, na.rm = TRUE) -->
<!--             ,avg_3P = median(`3P`, na.rm = TRUE) -->
<!--             ,avg_3PA = median(`3PA`, na.rm = TRUE) -->
<!--             #,`avg_3P%` = avg_3P / avg_3PA -->
<!--             ,avg_2P = median(`2P`, na.rm = TRUE) -->
<!--             ,avg_2PA = median(`2PA`, na.rm = TRUE) -->
<!--             #,`avg_2P%` = avg_2P / avg_2PA -->
<!--             ,avg_FT = median(FT, na.rm = TRUE) -->
<!--             ,avg_FTA = median(FTA, na.rm = TRUE) -->
<!--             #,`avg_FT%` = avg_FT / avg_FTA -->
<!--             ,avg_ORB = median(ORB, na.rm = TRUE) -->
<!--             ,avg_DRB = median(DRB, na.rm = TRUE) -->
<!--             #,avg_TRB = median(TRB, na.rm = TRUE) -->
<!--             ,avg_AST = median(AST, na.rm = TRUE) -->
<!--             ,avg_STL = median(STL, na.rm = TRUE) -->
<!--             ,avg_BLK = median(BLK, na.rm = TRUE) -->
<!--             ,avg_TOV = median(TOV, na.rm = TRUE) -->
<!--             #,avg_PF = median(PF, na.rm = TRUE) -->
<!--             #,avg_POINTS = median(POINTS, na.rm = TRUE) -->
<!--             ,avg_GP = median(GP, na.rm = TRUE) -->
<!--             ,avg_MPG = median(MPG, na.rm = TRUE) -->
<!--             #,avg_ORPM = median(ORPM, na.rm = TRUE) -->
<!--             #,avg_DRPM = median(DRPM, na.rm = TRUE) -->
<!--             #,avg_RPM = median(RPM, na.rm = TRUE) -->
<!--             #,avg_WINS_RPM = median(WINS_RPM, na.rm = TRUE) -->
<!--             #,avg_PIE = median(PIE, na.rm = TRUE) -->
<!--             ,avg_PACE = median(PACE, na.rm = TRUE) -->
<!--             ,avg_W = median(W, na.rm = TRUE)) %>%  -->
<!--   right_join(team_valuations, by = c('TEAM.x' = 'TEAM')) %>%  -->
<!--   mutate(TEAM.x = factor(TEAM.x)) -->

<!-- # Remove qualitative columns and data which has a high correlation -->
<!-- salary_valuations_by_team2 <- salary_valuations_by_team2 %>%  -->
<!--   # original select... commented out because we may be facing the curse -->
<!--   # of dimensionality -->
<!--   select(-TEAM.x, -median_salary, -avg_FGA, -avg_3PA, -avg_2PA, -avg_FTA) -->
<!-- # select(VALUE_MILLIONS, total_salary, mean_salary, low_salary, high_salary) -->

<!-- # Create variable to hold the number of predictors -->
<!-- num_predictors <- dim(salary_valuations_by_team2)[2] - 1 -->

<!-- # Perform best-subsets  -->
<!-- val_from_salary_model <- regsubsets(VALUE_MILLIONS ~ . -->
<!--                                     ,data = salary_valuations_by_team2 -->
<!--                                     ,nvmax = num_predictors -->
<!--                                     ,method = 'forward') -->

<!-- val_from_salary_summary <- summary(val_from_salary_model) -->

<!-- # Create tibble which contains data from results object -->
<!-- val_from_salary_results <- tibble(num_pred = 1:num_predictors -->
<!--                                   ,rss = val_from_salary_summary$rss -->
<!--                                   ,rsquared = val_from_salary_summary$rsq -->
<!--                                   ,adj_rsquared = val_from_salary_summary$adjr2 -->
<!--                                   ,cp = val_from_salary_summary$cp -->
<!--                                   ,bic = val_from_salary_summary$bic) -->

<!-- # RSS -->
<!-- plot1 <- val_from_salary_results %>%  -->
<!--   ggplot(aes(num_pred, rss)) +  -->
<!--   geom_point() + -->
<!--   geom_vline(aes(xintercept = which.min(val_from_salary_results$rss)), color = 'red') + -->
<!--   xlab('Number of Predictors') + -->
<!--   ylab('RSS') -->

<!-- # ADJ R-SQUARED -->
<!-- plot2 <- val_from_salary_results %>%  -->
<!--   ggplot(aes(num_pred, adj_rsquared)) +  -->
<!--   geom_point() + -->
<!--   geom_vline(aes(xintercept = which.max(val_from_salary_results$adj_rsquared)), color = 'red') + -->
<!--   xlab('Number of Predictors') + -->
<!--   ylab('Adj R-squared') -->

<!-- # CP -->
<!-- plot3 <- val_from_salary_results %>%  -->
<!--   ggplot(aes(num_pred, cp)) +  -->
<!--   geom_point() + -->
<!--   geom_vline(aes(xintercept = which.min(val_from_salary_results$cp)), color = 'red') + -->
<!--   xlab('Number of Predictors') + -->
<!--   ylab('Cp') -->

<!-- # BIC -->
<!-- plot4 <- val_from_salary_results %>%  -->
<!--   ggplot(aes(num_pred, bic)) +  -->
<!--   geom_point() + -->
<!--   geom_vline(aes(xintercept = which.min(val_from_salary_results$bic)), color = 'red') + -->
<!--   xlab('Number of Predictors') + -->
<!--   ylab('BIC') -->

<!-- # Each of the measures comes up with vastly different number of  -->
<!-- # predictors...  -->
<!-- grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2) -->
<!-- ``` -->

<!-- ### $%*&! Looks weird. Done. -->
<br>
<br>